# ============================================================================
# AWS ParallelCluster Infrastructure Configuration
# Complete example with all configurable options
# ============================================================================

# ============================================================================
# Basic Configuration
# ============================================================================
aws_region   = "us-east-2"
cluster_name = "my-hpc-cluster"
key_name     = "sa-malibo-hpc-east-2"  # REQUIRED: Replace with your EC2 key pair name

# ============================================================================
# VPC and Networking Configuration
# ============================================================================

# VPC Configuration
vpc_cidr = "10.0.0.0/16"  # Main VPC CIDR block - adjust as needed

# Availability Zones (must have at least 3 for high availability)
availability_zones = ["us-east-2a", "us-east-2b", "us-east-2c"]
# Alternative examples:
# availability_zones = ["us-west-2a", "us-west-2b", "us-west-2c"]  # For us-west-2
# availability_zones = ["us-east-1a", "us-east-1b", "us-east-1c"]  # For us-east-1

# Public Subnets (for head node, NAT gateways, and internet access)
public_subnet_cidrs = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
# Each subnet gets /24 (254 usable IPs) - adjust sizes as needed:
# public_subnet_cidrs = ["10.0.1.0/26", "10.0.2.0/26", "10.0.3.0/26"]  # Smaller subnets (62 IPs each)
# public_subnet_cidrs = ["10.0.1.0/23", "10.0.2.0/23", "10.0.4.0/23"]  # Larger subnets (510 IPs each)

# Private Subnets (for compute nodes and storage)
private_subnet_cidrs = ["10.0.11.0/24", "10.0.12.0/24", "10.0.13.0/24"]
# Compute-optimized examples:
# private_subnet_cidrs = ["10.0.11.0/22", "10.0.15.0/24", "10.0.16.0/24"]  # Large first subnet for many compute nodes
# private_subnet_cidrs = ["10.0.11.0/20", "10.0.32.0/24", "10.0.33.0/24"]  # Very large first subnet (4094 IPs)

# Compute Subnet Selection
compute_subnet_index = 0  # Use first private subnet (10.0.11.0/24) for ALL compute nodes
# Options:
# compute_subnet_index = 1  # Use second private subnet
# compute_subnet_index = 2  # Use third private subnet

# Security Configuration
allowed_ssh_cidr = "203.0.113.0/24"  # SECURITY: Replace with your IP range
# Examples:
# allowed_ssh_cidr = "0.0.0.0/0"        # Allow from anywhere (NOT recommended for production)
# allowed_ssh_cidr = "10.0.0.0/8"       # Allow from private networks only
# allowed_ssh_cidr = "192.168.1.0/24"   # Allow from specific local network

# ============================================================================
# Head Node Configuration
# ============================================================================
os_image                 = "ubuntu2204"  # ubuntu2204, ubuntu2404, alinux2, centos7, rhel8, rhel9
head_node_instance_type  = "g6.xlarge"   # GPU-enabled head node for visualization
enable_dcv              = true           # Enable DCV remote desktop
root_volume_size        = 120            # GB
root_volume_type        = "gp3"          # gp2, gp3, io1, io2

# ============================================================================
# Compute Queues Configuration (Based on Your Template)
# ============================================================================
compute_queues = {
  # High-Performance GPU Queue for training/HPC workloads (H100 GPUs)
  "high-gpu-queue" = {
    instance_types = ["p5.4xlarge"]  # H100 GPUs - Latest and most powerful
    min_count     = 0
    max_count     = 4                # Match your template
    root_volume_size = 120
    root_volume_type = "gp3"
    enable_placement_group = true
    capacity_reservation_id = null   # Add your capacity reservation ID if available
  }
  
  # GPU Queue for inference workloads (L4 GPUs)
  "gpu-queue-inference" = {
    instance_types = ["g6f.2xlarge"]  # L4 GPUs - Cost-effective inference
    min_count     = 0
    max_count     = 10               # Match your template
    root_volume_size = 120
    root_volume_type = "gp3"
    enable_placement_group = true
    capacity_reservation_id = null
  }
  
  # High CPU Queue for compute-intensive workloads
  "cpu-queue-high" = {
    instance_types = ["c7i.16xlarge"]  # 64 vCPUs, 128 GiB memory, 25Gbps network
    min_count     = 0
    max_count     = 50               # Match your template
    root_volume_size = 120
    root_volume_type = "gp3"
    enable_placement_group = false
    capacity_reservation_id = null
  }
  
  # Default CPU Queue for general workloads
  "cpu-queue-default" = {
    instance_types = ["c7i.xlarge"]   # 4 vCPUs, 8 GiB memory - Match your template
    min_count     = 0
    max_count     = 50               # Match your template
    root_volume_size = 120
    root_volume_type = "gp3"
    enable_placement_group = false
    capacity_reservation_id = null
  }
}

# ============================================================================
# Additional Queue Examples (Uncomment and modify as needed)
# ============================================================================

# # Memory-optimized queue for large datasets
# "memory-queue" = {
#   instance_types = ["r7i.4xlarge", "r7i.8xlarge"]  # High memory instances
#   min_count     = 0
#   max_count     = 20
#   root_volume_size = 200
#   root_volume_type = "gp3"
#   enable_placement_group = false
#   capacity_reservation_id = null
# }

# # Mixed GPU queue with multiple instance types
# "gpu-mixed-queue" = {
#   instance_types = ["g6f.2xlarge", "g6f.4xlarge", "g6f.8xlarge"]  # Multiple L4 sizes
#   min_count     = 0
#   max_count     = 15
#   root_volume_size = 150
#   root_volume_type = "gp3"
#   enable_placement_group = true
#   capacity_reservation_id = null
# }

# # A100 GPU queue (if you have access)
# "gpu-a100-queue" = {
#   instance_types = ["p4d.24xlarge"]  # A100 GPUs - 8x A100 40GB
#   min_count     = 0
#   max_count     = 5
#   root_volume_size = 200
#   root_volume_type = "gp3"
#   enable_placement_group = true
#   capacity_reservation_id = "cr-06ec517ae4a3d013e"  # Add your capacity reservation
# }

# # Graviton-based ARM instances for cost optimization
# "arm-cpu-queue" = {
#   instance_types = ["c7g.2xlarge", "c7g.4xlarge"]  # ARM-based Graviton3
#   min_count     = 0
#   max_count     = 30
#   root_volume_size = 120
#   root_volume_type = "gp3"
#   enable_placement_group = false
#   capacity_reservation_id = null
# }

# ============================================================================
# Shared Storage Configuration
# ============================================================================

# EFS Configuration (Primary shared storage)
enable_efs                    = true
efs_performance_mode          = "generalPurpose"  # generalPurpose or maxIO
efs_throughput_mode          = "provisioned"      # bursting or provisioned
efs_provisioned_throughput   = 500               # MiB/s (only for provisioned mode)
efs_mount_dir               = "/shared"

# FSx Lustre Configuration (Optional high-performance storage)
enable_fsx_lustre           = false              # Set to true to enable
fsx_storage_capacity        = 1200               # GB (1200, 2400, 4800, etc.)
fsx_deployment_type         = "SCRATCH_2"        # SCRATCH_1, SCRATCH_2, PERSISTENT_1, PERSISTENT_2
fsx_per_unit_storage_throughput = 200           # MB/s/TiB (50, 100, 200)
fsx_mount_dir              = "/fsx"
fsx_s3_import_path         = null               # "s3://my-bucket/data/" (optional)
fsx_s3_export_path         = null               # "s3://my-bucket/results/" (optional)

# ============================================================================
# IAM and Security Configuration
# ============================================================================
additional_iam_policies = [
  "arn:aws:iam::aws:policy/AmazonS3FullAccess",
  "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore",
  "arn:aws:iam::aws:policy/SecretsManagerReadWrite"
  # Add custom policies as needed:
  # "arn:aws:iam::123456789012:policy/MyCustomPolicy"
]

# ============================================================================
# Monitoring and Logging Configuration
# ============================================================================
enable_detailed_monitoring     = false          # Enable detailed CloudWatch monitoring
enable_cloudwatch_logs        = true           # Enable CloudWatch logs
cloudwatch_log_retention_days = 14             # Log retention period (1, 3, 5, 7, 14, 30, 60, 90, etc.)

# ============================================================================
# Resource Tagging
# ============================================================================
additional_tags = {
  Environment = "production"
  Project     = "ParallelCluster"
  Owner       = "hpc-for-sansheng"
  CostCenter  = "engineering"
  Department  = "research"
  # Add more tags as needed for your organization
}

# ============================================================================
# Instance Type Reference Guide
# ============================================================================
# 
# GPU Instances:
# - p5.4xlarge:   1x H100 (80GB), 16 vCPUs, 128 GiB RAM  - Latest, most powerful
# - p4d.24xlarge: 8x A100 (40GB), 96 vCPUs, 1152 GiB RAM - High performance training
# - g6f.2xlarge:  1x L4 (24GB), 8 vCPUs, 32 GiB RAM      - Cost-effective inference
# - g6f.4xlarge:  1x L4 (24GB), 16 vCPUs, 64 GiB RAM     - Inference with more CPU
# - g6f.8xlarge:  1x L4 (24GB), 32 vCPUs, 128 GiB RAM    - Inference with high CPU
#
# CPU Instances:
# - c7i.xlarge:    4 vCPUs, 8 GiB RAM    - General purpose
# - c7i.2xlarge:   8 vCPUs, 16 GiB RAM   - Standard compute
# - c7i.4xlarge:   16 vCPUs, 32 GiB RAM  - Medium compute
# - c7i.8xlarge:   32 vCPUs, 64 GiB RAM  - High compute
# - c7i.16xlarge:  64 vCPUs, 128 GiB RAM - Very high compute
# - c7i.24xlarge:  96 vCPUs, 192 GiB RAM - Maximum compute
#
# Memory-Optimized Instances:
# - r7i.2xlarge:   8 vCPUs, 64 GiB RAM   - High memory
# - r7i.4xlarge:   16 vCPUs, 128 GiB RAM - Very high memory
# - r7i.8xlarge:   32 vCPUs, 256 GiB RAM - Extreme memory
#
# ARM-based Instances (Graviton3):
# - c7g.2xlarge:   8 vCPUs, 16 GiB RAM   - ARM-based, cost-effective
# - c7g.4xlarge:   16 vCPUs, 32 GiB RAM  - ARM-based, higher performance
#
# ============================================================================
#
 ============================================================================
# Advanced VPC Configuration Options
# ============================================================================

# DNS Configuration
enable_dns_hostnames = true   # Enable DNS hostnames (required for ParallelCluster)
enable_dns_support   = true   # Enable DNS support (required for ParallelCluster)

# Public Subnet Options
map_public_ip_on_launch = true  # Auto-assign public IPs in public subnets

# NAT Gateway Configuration (for private subnet internet access)
enable_nat_gateway   = true   # Enable NAT Gateways (required for compute nodes)
single_nat_gateway   = false  # Use one NAT Gateway per AZ (recommended for production)
# single_nat_gateway = true   # Use single NAT Gateway for cost optimization (dev/test)

# VPN Gateway (optional - for hybrid cloud connectivity)
enable_vpn_gateway = false    # Enable if you need VPN connectivity to on-premises

# ============================================================================
# Network Architecture Examples
# ============================================================================

# Example 1: Large Compute Subnet (for many compute nodes)
# vpc_cidr = "10.0.0.0/16"
# public_subnet_cidrs  = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]      # 254 IPs each
# private_subnet_cidrs = ["10.0.16.0/20", "10.0.32.0/24", "10.0.33.0/24"]   # 4094, 254, 254 IPs
# compute_subnet_index = 0  # Use the large /20 subnet for compute

# Example 2: Multi-Region Setup (different CIDR ranges)
# vpc_cidr = "172.16.0.0/16"  # Use 172.16.x.x range
# public_subnet_cidrs  = ["172.16.1.0/24", "172.16.2.0/24", "172.16.3.0/24"]
# private_subnet_cidrs = ["172.16.11.0/24", "172.16.12.0/24", "172.16.13.0/24"]

# Example 3: Small Development Environment
# vpc_cidr = "192.168.0.0/24"  # Small VPC for dev/test
# public_subnet_cidrs  = ["192.168.0.0/27", "192.168.0.32/27", "192.168.0.64/27"]    # 30 IPs each
# private_subnet_cidrs = ["192.168.0.96/27", "192.168.0.128/27", "192.168.0.160/27"] # 30 IPs each

# Example 4: Cost-Optimized (single NAT Gateway)
# single_nat_gateway = true  # Use only one NAT Gateway to save costs

# ============================================================================
# Subnet Sizing Guide
# ============================================================================
# /20 = 4094 usable IPs (recommended for large compute clusters)
# /21 = 2046 usable IPs (good for medium clusters)
# /22 = 1022 usable IPs (good for small-medium clusters)
# /23 = 510 usable IPs  (good for small clusters)
# /24 = 254 usable IPs  (standard, good for most use cases)
# /25 = 126 usable IPs  (small subnets)
# /26 = 62 usable IPs   (very small subnets)
# /27 = 30 usable IPs   (minimal subnets for dev/test)

# ============================================================================
# Region-Specific Availability Zone Examples
# ============================================================================
# us-east-1: ["us-east-1a", "us-east-1b", "us-east-1c", "us-east-1d", "us-east-1f"]
# us-east-2: ["us-east-2a", "us-east-2b", "us-east-2c"]
# us-west-1: ["us-west-1a", "us-west-1c"]
# us-west-2: ["us-west-2a", "us-west-2b", "us-west-2c", "us-west-2d"]
# eu-west-1: ["eu-west-1a", "eu-west-1b", "eu-west-1c"]
# ap-southeast-1: ["ap-southeast-1a", "ap-southeast-1b", "ap-southeast-1c"]